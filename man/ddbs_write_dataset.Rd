% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ddbs_write_dataset.R
\name{ddbs_write_dataset}
\alias{ddbs_write_dataset}
\title{Write spatial dataset to disk}
\usage{
ddbs_write_dataset(
  data,
  path,
  format = NULL,
  conn = NULL,
  overwrite = FALSE,
  crs = NULL,
  options = list(),
  partitioning = if (inherits(data, c("tbl_lazy", "duckspatial_df")))
    dplyr::group_vars(data) else NULL,
  parquet_compression = NULL,
  parquet_row_group_size = NULL,
  layer_creation_options = NULL,
  quiet = FALSE
)
}
\arguments{
\item{data}{A \code{duckspatial_df}, \code{tbl_lazy} (DuckDB), \code{sf} object, or \code{data.frame}.}

\item{path}{Path to output file.}

\item{format}{Output format. If \code{NULL} (default), inferred from file extension.
Common writable formats include:
\itemize{
\item Native: "parquet", "csv"
\item GDAL Spatial: "ESRI Shapefile" (or "shp"), "GPKG" (or "gpkg"), "GeoJSON" (or "geojson"),
"FlatGeoBuf" (or "fgb"), "KML", "GPX", "GML", "SQLite"
}

Run \code{ddbs_drivers()} to see all GDAL drivers available on your system (check \code{can_create} column
for writable formats). The exact set of available drivers may vary by DuckDB installation.}

\item{conn}{DuckDB connection. If \code{NULL}, attempts to infer from \code{data} or uses default connection.}

\item{overwrite}{Logical. If \code{TRUE}, overwrites existing file.
For Parquet/CSV, this uses DuckDB's \code{OVERWRITE_OR_IGNORE} flag.
For GDAL formats, the file is explicitly deleted before writing to ensure consistent behavior.}

\item{crs}{Output Coordinate Reference System (e.g., "EPSG:4326").
If provided, passed to GDAL as \code{SRS} option (overrides data CRS).
Ignored for Parquet.}

\item{options}{Named list of additional options passed to \code{COPY}.}

\item{partitioning}{Character vector of column names to partition by (Parquet/CSV only).
If \code{NULL} (default), uses \code{dplyr::group_vars(data)} if available.}

\item{parquet_compression}{Compression codec for Parquet (e.g., "ZSTD", "SNAPPY", "GZIP", "UNCOMPRESSED").}

\item{parquet_row_group_size}{Row group size for Parquet (integer).}

\item{layer_creation_options}{GDAL layer creation options (e.g., "WRITE_BBOX=YES").}

\item{quiet}{Logical. if \code{TRUE}, suppresses success message.}
}
\value{
The \code{path} invisibly.
}
\description{
Writes a \code{duckspatial_df}, \code{tbl_lazy} (DuckDB), or \code{sf} object to a file
using DuckDB's \code{COPY} command. Supports Parquet (native) and various spatial
formats via GDAL (GeoJSON, GeoPackage, etc.).
}
\examples{
\dontrun{
library(duckspatial)

# Read example data
path <- system.file("spatial/countries.geojson", package = "duckspatial")
ds <- ddbs_open_dataset(path)

# Write to Parquet
tmp_parquet <- tempfile(fileext = ".parquet")
ddbs_write_dataset(ds, tmp_parquet)

# Write to GeoJSON with CRS override
tmp_json <- tempfile(fileext = ".geojson")
ddbs_write_dataset(ds, tmp_json, crs = "EPSG:3857")

# Write to GeoPackage with overwrite
tmp_gpkg <- tempfile(fileext = ".gpkg")
ddbs_write_dataset(ds, tmp_gpkg, overwrite = TRUE)
}
}
\references{
This function is inspired by and builds upon the logic found in the
\code{duckdbfs} package (\url{https://github.com/cboettig/duckdbfs}),
particularly its \code{write_dataset} and \code{write_geo} functions.
For advanced features like cloud storage (S3) support, the
\code{duckdbfs} package is highly recommended.
}
\seealso{
\code{\link[=ddbs_drivers]{ddbs_drivers()}} to list all available GDAL drivers and formats on your system.
}
